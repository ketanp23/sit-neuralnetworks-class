{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwdSqpCfbJ7ECyFUKHOsPL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ketanp23/sit-neuralnetworks-class/blob/main/Rosenblatt_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Rosenblatt Perceptron is a foundational algorithm in machine learning, specifically for binary classification, and can be implemented in Python. It is a type of single-layer artificial neural network.\n",
        "\n",
        "Core Components and Implementation in Python:\n",
        "\n",
        "Initialization:\n",
        "Weights (w): A vector of numerical values, typically initialized to small random numbers or zeros. These weights represent the strength of connection between inputs and the output.\n",
        "\n",
        "Bias (b): A single numerical value, also initialized similarly, which shifts the activation function's threshold.\n",
        "\n",
        "Learning Rate (lr): A small positive value that controls the magnitude of weight adjustments during training.\n",
        "\n",
        "Epochs: The number of times the algorithm will iterate through the entire training dataset."
      ],
      "metadata": {
        "id": "RxpwuaTDlfkk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction (Activation Function):\n",
        "\n",
        "The perceptron calculates a weighted sum of its inputs, including the bias term. This is often represented as a dot product of the input features and the weights, plus the bias: z = np.dot(X, self.weights) + self.bias.\n",
        "This sum z is then passed through an activation function, typically a step function (like the Heaviside step function), which outputs 1 if z exceeds a certain threshold (or 0 if z is less than or equal to the threshold), and 0 otherwise.\n",
        "\n",
        "\n",
        "\n",
        "Training (Weight Update Rule):\n",
        "\n",
        "The perceptron learns by adjusting its weights based on the difference between its predicted output and the true target label.\n",
        "If the prediction is incorrect, the weights and bias are updated using the\n",
        "\n",
        "Perceptron Learning Rule:\n",
        "self.weights += self.lr * (target - prediction) * X\n",
        "self.bias += self.lr * (target - prediction)\n",
        "This process is repeated for a specified number of epochs or until convergence (for linearly separable data).\n",
        "\n"
      ],
      "metadata": {
        "id": "2t3w9NcLluCJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rDxpEyoblWe1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def _step_function(self, x):\n",
        "        return np.where(x >= 0, 1, 0)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
        "                y_predicted = self._step_function(linear_output)\n",
        "\n",
        "                update = self.learning_rate * (y[idx] - y_predicted)\n",
        "                self.weights += update * x_i\n",
        "                self.bias += update\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self._step_function(linear_output)\n",
        "        return y_predicted\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage (AND gate)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "p = Perceptron(learning_rate=0.1, epochs=10)\n",
        "p.fit(X, y)\n",
        "predictions = p.predict(X)\n",
        "\n",
        "print(\"Predictions for AND gate:\", predictions)\n",
        "# Expected output: [0 0 0 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKl-er-Im4n5",
        "outputId": "78a2c73a-2abf-48d3-de91-5a3a109c3c04"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions for AND gate: [0 0 0 1]\n"
          ]
        }
      ]
    }
  ]
}