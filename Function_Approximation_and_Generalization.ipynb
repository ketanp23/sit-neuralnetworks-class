{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ketanp23/sit-neuralnetworks-class/blob/main/Function_Approximation_and_Generalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. Define the True Underlying Function ---\n",
        "def target_function(x):\n",
        "    \"\"\"The function we are trying to approximate: f(x) = 0.5 * sin(2*pi*x) + x\"\"\"\n",
        "    return 0.5 * np.sin(2 * np.pi * x) + x\n",
        "\n",
        "# --- 2. Data Generation ---\n",
        "N_SAMPLES = 100\n",
        "NOISE_STD = 0.1 # Standard deviation of Gaussian noise\n",
        "\n",
        "# Generate 100 random input points between 0 and 1\n",
        "X_train_np = np.sort(np.random.rand(N_SAMPLES, 1), axis=0)\n",
        "\n",
        "# Calculate true y values and add noise\n",
        "Y_true_np = target_function(X_train_np)\n",
        "Y_train_np = Y_true_np + np.random.normal(0, NOISE_STD, Y_true_np.shape)\n",
        "\n",
        "# Convert NumPy arrays to PyTorch Tensors\n",
        "X_train = torch.tensor(X_train_np, dtype=torch.float32)\n",
        "Y_train = torch.tensor(Y_train_np, dtype=torch.float32)\n",
        "\n",
        "print(f\"Generated {N_SAMPLES} noisy training samples.\")\n",
        "\n",
        "# --- 3. Define the Neural Network Model ---\n",
        "class FunctionApproximator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Input layer (1 feature: x) -> Hidden layer 1\n",
        "        self.fc1 = nn.Linear(1, 50)\n",
        "        # Hidden layer 1 -> Hidden layer 2\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        # Hidden layer 2 -> Output layer (1 output: y)\n",
        "        self.fc3 = nn.Linear(50, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss, and optimizer\n",
        "model = FunctionApproximator()\n",
        "criterion = nn.MSELoss() # Mean Squared Error is standard for regression/approximation\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# --- 4. Training Loop ---\n",
        "NUM_EPOCHS = 5000\n",
        "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Forward pass\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, Y_train)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {loss.item():.6f}')\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# --- 5. Test for Generalization and Visualization ---\n",
        "\n",
        "# Generate clean, dense data points for testing generalization (unseen data)\n",
        "# This allows us to see the smoothness of the learned function vs. the true function.\n",
        "X_test_np = np.linspace(0, 1, 500).reshape(-1, 1)\n",
        "Y_true_clean_np = target_function(X_test_np)\n",
        "X_test = torch.tensor(X_test_np, dtype=torch.float32)\n",
        "\n",
        "# Make predictions using the trained model\n",
        "model.eval() # Set model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    Y_pred = model(X_test).numpy()\n",
        "\n",
        "# Calculate the error on the clean test set (generalization error)\n",
        "generalization_loss = np.mean((Y_pred - Y_true_clean_np)**2)\n",
        "\n",
        "# --- 6. Plotting the Results ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot 1: The True Function (for comparison)\n",
        "plt.plot(X_test_np, Y_true_clean_np, label='True Function: $f(x)$', color='blue', linewidth=3, linestyle='--')\n",
        "\n",
        "# Plot 2: The Noisy Training Data (what the model saw)\n",
        "plt.scatter(X_train_np, Y_train_np, label='Noisy Training Data', color='red', alpha=0.6, s=20)\n",
        "\n",
        "# Plot 3: The Learned/Approximated Function (The generalization result)\n",
        "plt.plot(X_test_np, Y_pred, label='Learned Function (Approximation)', color='green', linewidth=3)\n",
        "\n",
        "plt.title(f'Function Approximation and Generalization (MSE on Test: {generalization_loss:.4e})', fontsize=14)\n",
        "plt.xlabel('Input X', fontsize=12)\n",
        "plt.ylabel('Output Y', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--')\n",
        "plt.savefig('function_approximation_result.png')\n",
        "plt.close()\n",
        "\n",
        "# --- 7. Print Final Output ---\n",
        "print(\"-\" * 50)\n",
        "print(f\"Final Training Loss: {loss.item():.6f}\")\n",
        "print(f\"Generalization MSE on Clean Test Data: {generalization_loss:.4e}\")\n",
        "print(\"\\nVisualization saved to 'function_approximation_result.png'\")\n",
        "print(\"The plot demonstrates generalization: the green line (Learned Function) fits the blue line (True Function) well, ignoring the scattered red dots (Noise).\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 100 noisy training samples.\n",
            "Starting training for 5000 epochs...\n",
            "Epoch [1000/5000], Loss: 0.007523\n",
            "Epoch [2000/5000], Loss: 0.007335\n",
            "Epoch [3000/5000], Loss: 0.007192\n",
            "Epoch [4000/5000], Loss: 0.007105\n",
            "Epoch [5000/5000], Loss: 0.007041\n",
            "Training finished.\n",
            "--------------------------------------------------\n",
            "Final Training Loss: 0.007041\n",
            "Generalization MSE on Clean Test Data: 1.6891e-03\n",
            "\n",
            "Visualization saved to 'function_approximation_result.png'\n",
            "The plot demonstrates generalization: the green line (Learned Function) fits the blue line (True Function) well, ignoring the scattered red dots (Noise).\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54MFrENtc6W3",
        "outputId": "0f571203-7e2d-4b17-81bb-b4a0891769d2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}