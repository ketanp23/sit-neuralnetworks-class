{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ketanp23/sit-neuralnetworks-class/blob/main/Network_Pruning_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.nn.utils import prune\n",
        "import ssl\n",
        "# Temporarily disable SSL verification for dataset download if needed\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# --- 1. Define the Simple CNN Model ---\n",
        "class PrunableNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PrunableNet, self).__init__()\n",
        "        # Define two convolutional layers and two fully connected layers\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "\n",
        "        # Output size after two pools: (28 - 4) / 2 = 12 -> (12 - 4) / 2 = 4\n",
        "        # 20 channels * 4 * 4\n",
        "        self.fc = nn.Linear(320, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(-1, 320) # Flatten\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# --- Helper function to calculate the sparsity of the model ---\n",
        "def calculate_sparsity(model):\n",
        "    \"\"\"Calculates the percentage of zero weights in the entire model.\"\"\"\n",
        "    total_params = 0\n",
        "    zero_params = 0\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "            # Check for both weight and bias, though pruning typically focuses on weight\n",
        "            for param_name in ['weight', 'bias']:\n",
        "                if hasattr(module, param_name):\n",
        "                    param = getattr(module, param_name)\n",
        "                    total_params += param.numel()\n",
        "                    zero_params += torch.sum(param == 0).item()\n",
        "\n",
        "    if total_params == 0:\n",
        "        return 0.0\n",
        "\n",
        "    sparsity = 100. * zero_params / total_params\n",
        "    return sparsity\n",
        "\n",
        "# --- 2. Setup and Initial Training ---\n",
        "\n",
        "# Load Data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Initialize Model, Loss, and Optimizer\n",
        "model = PrunableNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "print(\"Starting initial training (1 Epoch)...\")\n",
        "# Train for a minimal period to give weights some value\n",
        "for epoch in range(1):\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 500 == 499:\n",
        "            print(f'  [Epoch {epoch + 1}, Batch {i + 1:5d}] Loss: {loss.item():.4f}')\n",
        "\n",
        "print(\"Initial training complete.\")\n",
        "initial_sparsity = calculate_sparsity(model)\n",
        "print(f\"Sparsity before pruning: {initial_sparsity:.2f}%\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- 3. Apply Network Pruning ---\n",
        "\n",
        "# Define the layers we want to prune\n",
        "parameters_to_prune = [\n",
        "    (model.conv1, 'weight'),\n",
        "    (model.conv2, 'weight'),\n",
        "    (model.fc, 'weight'),\n",
        "]\n",
        "\n",
        "# Apply L1 Unstructured Pruning: remove the lowest 40% magnitude weights globally\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.40, # Prune 40% of the weights\n",
        ")\n",
        "\n",
        "print(\"Pruning applied: Removed 40% of the lowest magnitude weights across Conv and FC layers.\")\n",
        "\n",
        "# --- 4. Evaluate Pruning Result ---\n",
        "\n",
        "# Check sparsity after pruning\n",
        "pruned_sparsity = calculate_sparsity(model)\n",
        "print(f\"Sparsity after pruning: {pruned_sparsity:.2f}%\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- 5. Verify Model Status (Optional Fine-tuning) ---\n",
        "\n",
        "# To confirm the pruning operation introduced masks, check the weight attribute:\n",
        "print(\"Verifying pruning status for model.conv1.weight:\")\n",
        "print(f\"  Has a 'weight_orig' attribute: {hasattr(model.conv1, 'weight_orig')}\")\n",
        "print(f\"  Has a 'weight_mask' attribute: {hasattr(model.conv1, 'weight_mask')}\")\n",
        "\n",
        "# To make the pruning permanent (remove the mask and the original weight), use remove:\n",
        "# prune.remove(model.conv1, 'weight')\n",
        "# print(\"\\nMask removed from conv1.weight.\")\n",
        "\n",
        "# --- Demonstration of Fine-tuning (Optional, but shows the next step) ---\n",
        "# Fine-tuning is typically done after pruning to recover lost accuracy.\n",
        "print(\"\\nStarting Fine-tuning (1 Epoch) on the pruned model...\")\n",
        "for epoch in range(1):\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        # The forward pass uses the masked/pruned weights automatically\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 500 == 499:\n",
        "            print(f'  [Epoch {epoch + 1}, Batch {i + 1:5d}] Loss: {loss.item():.4f}')\n",
        "\n",
        "print(\"Fine-tuning complete. The pruned model is now trained using only 60% of its original weights.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 24.1MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 594kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 5.47MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.79MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting initial training (1 Epoch)...\n",
            "  [Epoch 1, Batch   500] Loss: 0.1497\n",
            "Initial training complete.\n",
            "Sparsity before pruning: 0.00%\n",
            "--------------------------------------------------\n",
            "Pruning applied: Removed 40% of the lowest magnitude weights across Conv and FC layers.\n",
            "Sparsity after pruning: 39.81%\n",
            "--------------------------------------------------\n",
            "Verifying pruning status for model.conv1.weight:\n",
            "  Has a 'weight_orig' attribute: True\n",
            "  Has a 'weight_mask' attribute: True\n",
            "\n",
            "Starting Fine-tuning (1 Epoch) on the pruned model...\n",
            "  [Epoch 1, Batch   500] Loss: 0.0799\n",
            "Fine-tuning complete. The pruned model is now trained using only 60% of its original weights.\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4urt0emCjso0",
        "outputId": "25998b56-83d1-4e43-8ba8-dcd18f5790e9"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}